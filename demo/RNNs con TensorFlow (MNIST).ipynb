{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes con TensorFlow\n",
    "\n",
    "En este ejercicio vamos a realizar una primera experimentación con redes neuronales recurrentes en TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Usaremos un DataFrame de pandas para guardar la información de los experimentos\n",
    "import pandas as pd\n",
    "resultados = pd.DataFrame(columns=['modelo', 'num_units', 'num_epochs', 'exactitud (accuracy)', 'train_logdir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN *many-to-one* para clasificación de MNIST\n",
    "\n",
    "Si tratamos cada imagen MNIST de un alto *h* y ancho *w* como una secuencia de *h* vectores de *w* elementos, podemos emplear una red neuronal para predecir su clase, esto es, el dígito que representa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Cargar el conjunto de datos\n",
    "\n",
    "En primer lugar cargamos el conjunto de datos tal como lo hemos hecho antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Entrenamiento:  (55000, 28, 28, 1) (55000, 10)\n",
      "Validación:  (5000, 28, 28, 1) (5000, 10)\n",
      "Pruebas:  (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)\n",
    "\n",
    "print('Entrenamiento: ', mnist.train.images.shape, mnist.train.labels.shape)\n",
    "print('Validación: ', mnist.validation.images.shape, mnist.validation.labels.shape)\n",
    "print('Pruebas: ', mnist.test.images.shape, mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crear el grafo\n",
    "\n",
    "#### Dimensiones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# DIMENSIONES DEL MODELO\n",
    "#########################\n",
    "\n",
    "# Longitud de las secuencias: cada imagen tiene 28 filas (alto)\n",
    "num_steps = mnist.train.images.shape[1]\n",
    "\n",
    "# Capa de entrada: cada paso será una fila de 28 pixeles (ancho)\n",
    "input_size = mnist.train.images.shape[2]\n",
    "\n",
    "# Número de unidades en las unidades ocultas\n",
    "num_units = 128\n",
    "\n",
    "# Número de clases: 10 dígitos diferentes\n",
    "num_classes = mnist.train.labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensores para las entradas y parámetros\n",
    "\n",
    "Creamos los tensores para los datos de entrada y para los parámetros. Inicializamos los pesos $U$, $W$ y $V$ con el inicializador *Xavier*, esto es, con una distribución aleatoria uniforme en el rango $[-r, +r]$, donde $r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$. Inicializamos los biases $b$ y $c$ con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "####################\n",
    "# TENSORES\n",
    "####################\n",
    "\n",
    "# Datos de entrada\n",
    "with tf.name_scope(\"inputs\"):\n",
    "  x = tf.placeholder(tf.float32, [None, num_steps, input_size, 1], name=\"x\")\n",
    "  labels = tf.placeholder(tf.float32, [None, num_classes], name=\"labels\")\n",
    "  x_reshaped = tf.reshape(x, [-1, num_steps, input_size], name=\"x_reshaped\")\n",
    "\n",
    "# Parámetros del modelo (pesos y biases)\n",
    "with tf.variable_scope(\"parameters\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "  U = tf.get_variable(\"U\", [input_size, num_units])\n",
    "  W = tf.get_variable(\"W\", [num_units, num_units])\n",
    "  V = tf.get_variable(\"V\", [num_units, num_classes])\n",
    "  \n",
    "with tf.variable_scope(\"parameters\", initializer=tf.constant_initializer(0.)):\n",
    "  b = tf.get_variable(\"b\", [num_units])\n",
    "  c = tf.get_variable(\"c\", [num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones del modelo\n",
    "\n",
    "Definimos primero una función `next_h` para calcular cada $\\textbf{h}^{(t)}$ a partir de $\\textbf{h}^{(t-1)}$ y $\\textbf{x}^{(t)}$. \n",
    "\n",
    "$\\textbf{a}^{(t)}  = \\textbf{b} + \\textbf{W}\\textbf{h}^{(t-1)} + \\textbf{U}\\textbf{x}^{(t)}$\n",
    "\n",
    "$\\textbf{h}^{(t)}  = tanh(\\textbf{a}^{(t)})$\n",
    "\n",
    "En el código de abajo las multiplicaciones de matrices se encuentran invertidas en relacion a las ecuaciones porque estamos procesando en lotes las imágenes de entrada y TensorFlow espera recibir los datos con `batch_size` como la primera dimensión (no sería así si pusiésemos `batch_size` como última dimensión).\n",
    "\n",
    "Luego inicializamos $\\textbf{h}^{(0)}$ con ceros y ejecutamos el cálculo recurrente con el método `tf.scan`, que nos devuelve en la variable `h_outputs` los estados $\\textbf{h}^{(1)}, ..., \\textbf{h}^{(t)}$.\n",
    "\n",
    "Con el estado final `h_state` hacemos el cálculo de $o$ (`logits`), de las probabilidades normalizadas (`y_prob`) y de $\\hat{y}$ (`y_predictions`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# OPERACIONES DEL MODELO\n",
    "#########################\n",
    "\n",
    "def next_h(h_prev, x_t):\n",
    "  with tf.variable_scope(\"parameters\", reuse=True):\n",
    "    U = tf.get_variable(\"U\")\n",
    "    W = tf.get_variable(\"W\")\n",
    "    b = tf.get_variable(\"b\")\n",
    "  a_t = b + tf.matmul(h_prev, W) + tf.matmul(x_t, U)\n",
    "  h_t = tf.tanh(a_t)\n",
    "  return h_t\n",
    "\n",
    "with tf.name_scope(\"model\"):\n",
    "  with tf.name_scope(\"hidden\"):\n",
    "    h_0 = tf.zeros([tf.shape(x)[0], num_units], name=\"h_0\")\n",
    "    h_outputs = tf.scan(next_h, \n",
    "                        tf.transpose(x_reshaped, [1,0,2]),  # [num_steps, batch_size, num_inputs])\n",
    "                        initializer=h_0,\n",
    "                        name=\"h_outputs\"\n",
    "                       )\n",
    "    h_state = h_outputs[-1]\n",
    "    h_outputs = tf.transpose(h_outputs, [1,0,2], name=\"reshaped_h_outputs\")  # [batch_size, num_steps, num_units]\n",
    "  \n",
    "  with tf.name_scope(\"output\"):\n",
    "    logits = c + tf.matmul(h_state, V)\n",
    "    y_probs = tf.nn.softmax(logits)\n",
    "    y_predictions = tf.argmax(y_probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resto de operaciones son las mismas que cuando vimos MNIST con CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# OPERACIONES DE ENTRENAMIENTO\n",
    "#########################\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "  cost = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  loss = tf.reduce_mean(cost)\n",
    "  optimizer = tf.train.AdamOptimizer()\n",
    "  training_op = optimizer.minimize(loss)\n",
    "  \n",
    "\n",
    "#########################\n",
    "# OPERACIONES DE EVALUACIÓN Y VISUALIZACIÓN\n",
    "#########################\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "  y = tf.argmax(labels, axis=1)\n",
    "  correct = tf.nn.in_top_k(logits, y, 1)\n",
    "  accuracy = tf.reduce_mean(tf.to_float(correct))\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "  acc_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "  loss_summary = tf.summary.scalar('loss', loss)\n",
    "  merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Ejecutar el grafo\n",
    "\n",
    "Esta parte es también igual al modelo con CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Epoch  Train_loss  Train_acc   Val_loss    Val_acc\n",
      "-----------------------------------------------------\n",
      "0001   1.0620583   0.6875000   0.9877195   0.7188000\n",
      "0002   0.5944891   0.8256836   0.5333630   0.8492000\n",
      "0003   0.4114968   0.8740234   0.3769458   0.8932000\n",
      "0004   0.3136585   0.9091797   0.2986721   0.9128000\n",
      "0005   0.2907442   0.9135742   0.2538413   0.9280000\n",
      "0006   0.2451400   0.9287109   0.2169101   0.9400000\n",
      "0007   0.2288777   0.9306641   0.1956740   0.9460000\n",
      "0008   0.2024334   0.9321289   0.1784340   0.9524000\n",
      "0009   0.1483473   0.9609375   0.1668845   0.9524000\n",
      "0010   0.1442550   0.9609375   0.1499246   0.9604000\n",
      "0011   0.1632075   0.9531250   0.1427756   0.9624000\n",
      "0012   0.1496067   0.9584961   0.1368400   0.9620000\n",
      "0013   0.1152981   0.9677734   0.1298203   0.9652000\n",
      "0014   0.1594123   0.9521484   0.1269386   0.9656000\n",
      "0015   0.1228817   0.9589844   0.1178786   0.9692000\n",
      "0016   0.1226587   0.9648438   0.1165135   0.9690000\n",
      "0017   0.1045801   0.9677734   0.1103347   0.9704000\n",
      "0018   0.0939617   0.9746094   0.1120878   0.9700000\n",
      "0019   0.0871597   0.9775391   0.1065299   0.9734000\n",
      "0020   0.1122170   0.9692383   0.1020341   0.9726000\n",
      "-----------------------------------------------------\n",
      "Test accuracy: [0.96640003]\n",
      "=====================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>exactitud (accuracy)</th>\n",
       "      <th>train_logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN simple, many-to-one</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.9664]</td>\n",
       "      <td>/tmp/rnn-mnist/run-20171207015933-train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    modelo num_units num_epochs exactitud (accuracy)  \\\n",
       "0  RNN simple, many-to-one       128         20             [0.9664]   \n",
       "\n",
       "                              train_logdir  \n",
       "0  /tmp/rnn-mnist/run-20171207015933-train  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre_del_experimento = \"RNN simple, many-to-one\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "root_logdir = \"/tmp/rnn-mnist\" \n",
    "train_logdir = \"{}/run-{}-train\".format(root_logdir, now)\n",
    "val_logdir = \"{}/run-{}-val\".format(root_logdir, now)\n",
    "train_file_writer = tf.summary.FileWriter(train_logdir, tf.get_default_graph())\n",
    "val_file_writer = tf.summary.FileWriter(val_logdir)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"Epoch  Train_loss  Train_acc   Val_loss    Val_acc\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 2048\n",
    "num_batches_per_epoch = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess: \n",
    "  \n",
    "  tf.global_variables_initializer().run()\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    for step in range(num_batches_per_epoch):\n",
    "      cur_step = epoch * num_batches_per_epoch + step\n",
    "      batch_images, batch_labels = mnist.train.next_batch(batch_size)\n",
    "      _, train_loss, train_acc, summary = sess.run([training_op, loss, accuracy, merged], \n",
    "                                                     feed_dict={x: batch_images, labels: batch_labels})\n",
    "      train_file_writer.add_summary(summary, cur_step + 1)\n",
    "      \n",
    "    val_loss, val_acc, summary = sess.run([loss, accuracy, merged], \n",
    "                                          feed_dict={x: mnist.validation.images, labels: mnist.validation.labels})\n",
    "    val_file_writer.add_summary(summary, cur_step )\n",
    "    print(\"{:04d}{:12.7f}{:12.7f}{:12.7f}{:12.7f}\".format\n",
    "          (epoch+1, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "  test_acc = sess.run([accuracy], \n",
    "                      feed_dict={x: mnist.test.images, labels: mnist.test.labels})\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"=====================================================\")\n",
    "\n",
    "train_file_writer.close()\n",
    "val_file_writer.close()\n",
    "\n",
    "## Añadimos nuestros resultados al dataframe\n",
    "idx = len(resultados)\n",
    "resultados.loc[idx] = [nombre_del_experimento,\n",
    "                       num_units,\n",
    "                       num_epochs,\n",
    "                       test_acc,\n",
    "                       train_logdir ]\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Podemos visualizar el entrenamiento con el comando (corregir la letra del disco): \n",
    "\n",
    "`tensorboard --logdir=\"d:/tmp/rnn-mnist\"` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Refactorización del código para usar las funciones `BasicRNNCell` y `dynamic_rnn`\n",
    "\n",
    "La función [`tf.contrib.rnn.BasicRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell) crea una celda RNN básica como la que hemos usado y la función [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) crea una RNN compuesta del tipo de celdas que le sean indicadas, realizando un despliegue completamente dinámico de sus entradas:\n",
    "\n",
    "```python\n",
    "    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "    h_outputs, h_state = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=x_reshaped, dtype=tf.float32)\n",
    "```\n",
    "* Consolida el código de las secciones 1.2 y 1.3 en una celda cada uno.\n",
    "\n",
    "* Modifica el código para usar las funciones `BasicRNNCell` y `dynamic_rnn`. Ya no necesitas declarar manualmente los pesos `U` y `W`, ni el bias `b`. Deshabilita las líneas correspondientes anteponiéndoles un `#`\n",
    "\n",
    "* Tampoco necesitas la función `next_h`. Elimínala o deshabilítala también.\n",
    "\n",
    "* Finalmente reemplaza todo el código correspondiente al `name_scope(\"hidden\")` por las dos líneas de arriba y ejecuta el grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# DIMENSIONES DEL MODELO\n",
    "#########################\n",
    "\n",
    "# Longitud de las secuencias: cada imagen tiene 28 filas (alto)\n",
    "num_steps = mnist.train.images.shape[1]\n",
    "\n",
    "# Capa de entrada: cada paso será una fila de 28 pixeles (ancho)\n",
    "input_size = mnist.train.images.shape[2]\n",
    "\n",
    "# Número de unidades en las unidades ocultas\n",
    "num_units = 128\n",
    "\n",
    "# Número de clases: 10 dígitos diferentes\n",
    "num_classes = mnist.train.labels.shape[1]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "####################\n",
    "# TENSORES\n",
    "####################\n",
    "\n",
    "# Datos de entrada\n",
    "with tf.name_scope(\"inputs\"):\n",
    "  x = tf.placeholder(tf.float32, [None, num_steps, input_size, 1], name=\"x\")\n",
    "  labels = tf.placeholder(tf.float32, [None, num_classes], name=\"labels\")\n",
    "  x_reshaped = tf.reshape(x, [-1, num_steps, input_size], name=\"x_reshaped\")\n",
    "\n",
    "# Parámetros del modelo (pesos y biases)\n",
    "with tf.variable_scope(\"parameters\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "  U = tf.get_variable(\"U\", [input_size, num_units])\n",
    "  W = tf.get_variable(\"W\", [num_units, num_units])\n",
    "  V = tf.get_variable(\"V\", [num_units, num_classes])\n",
    "  \n",
    "with tf.variable_scope(\"parameters\", initializer=tf.constant_initializer(0.)):\n",
    "  b = tf.get_variable(\"b\", [num_units])\n",
    "  c = tf.get_variable(\"c\", [num_classes])\n",
    "  \n",
    "#########################\n",
    "# OPERACIONES DEL MODELO\n",
    "#########################\n",
    "\n",
    "'''\n",
    "def next_h(h_prev, x_t):\n",
    "  with tf.variable_scope(\"parameters\", reuse=True):\n",
    "    U = tf.get_variable(\"U\")\n",
    "    W = tf.get_variable(\"W\")\n",
    "    b = tf.get_variable(\"b\")\n",
    "  a_t = b + tf.matmul(h_prev, W) + tf.matmul(x_t, U)\n",
    "  h_t = tf.tanh(a_t)\n",
    "  return h_t\n",
    "'''\n",
    "\n",
    "with tf.name_scope(\"model\"):\n",
    "  with tf.name_scope(\"hidden\"):    \n",
    "    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "    h_outputs, h_state = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=x_reshaped, dtype=tf.float32)\n",
    "    '''\n",
    "    h_0 = tf.zeros([tf.shape(x)[0], num_units], name=\"h_0\")\n",
    "    h_outputs = tf.scan(next_h, \n",
    "                        tf.transpose(x_reshaped, [1,0,2]),  # [num_steps, batch_size, num_inputs])\n",
    "                        initializer=h_0,\n",
    "                        name=\"h_outputs\"\n",
    "                       )\n",
    "    h_state = h_outputs[-1]\n",
    "    h_outputs = tf.transpose(h_outputs, [1,0,2], name=\"reshaped_h_outputs\")  # [batch_size, num_steps, num_units]\n",
    "    '''\n",
    "    \n",
    "  with tf.name_scope(\"output\"):\n",
    "    logits = c + tf.matmul(h_state, V)\n",
    "    y_probs = tf.nn.softmax(logits)\n",
    "    y_predictions = tf.argmax(y_probs, axis=1)\n",
    "    \n",
    "#########################\n",
    "# OPERACIONES DE ENTRENAMIENTO\n",
    "#########################\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "  cost = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  loss = tf.reduce_mean(cost)\n",
    "  optimizer = tf.train.AdamOptimizer()\n",
    "  training_op = optimizer.minimize(loss)\n",
    "  \n",
    "\n",
    "#########################\n",
    "# OPERACIONES DE EVALUACIÓN Y VISUALIZACIÓN\n",
    "#########################\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "  y = tf.argmax(labels, axis=1)\n",
    "  correct = tf.nn.in_top_k(logits, y, 1)\n",
    "  accuracy = tf.reduce_mean(tf.to_float(correct))\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "  acc_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "  loss_summary = tf.summary.scalar('loss', loss)\n",
    "  merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Epoch  Train_loss  Train_acc   Val_loss    Val_acc\n",
      "-----------------------------------------------------\n",
      "0001   1.1381664   0.6240234   1.0820074   0.6514000\n",
      "0002   0.7580898   0.7500000   0.7451014   0.7700000\n",
      "0003   0.5452777   0.8354492   0.4775521   0.8640000\n",
      "0004   0.3907683   0.8842773   0.3636132   0.8986000\n",
      "0005   0.3534286   0.8916016   0.3013775   0.9138000\n",
      "0006   0.2963310   0.9174805   0.2670401   0.9248000\n",
      "0007   0.2944254   0.9184570   0.2409909   0.9300000\n",
      "0008   0.2633553   0.9267578   0.2340029   0.9330000\n",
      "0009   0.2081070   0.9409180   0.2138777   0.9402000\n",
      "0010   0.2108822   0.9375000   0.2217248   0.9362000\n",
      "0011   0.2147555   0.9418945   0.1900247   0.9444000\n",
      "0012   0.2078836   0.9345703   0.1852503   0.9464000\n",
      "0013   0.1706168   0.9467773   0.1732341   0.9506000\n",
      "0014   0.1983017   0.9394531   0.1671678   0.9530000\n",
      "0015   0.1534166   0.9541016   0.1698672   0.9536000\n",
      "0016   0.1594216   0.9521484   0.1643154   0.9512000\n",
      "0017   0.1743040   0.9482422   0.1545998   0.9552000\n",
      "0018   0.1220174   0.9619141   0.1443882   0.9578000\n",
      "0019   0.1653504   0.9531250   0.1438726   0.9580000\n",
      "0020   0.1270719   0.9614258   0.1374829   0.9618000\n",
      "-----------------------------------------------------\n",
      "Test accuracy: [0.9576]\n",
      "=====================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>exactitud (accuracy)</th>\n",
       "      <th>train_logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN simple, many-to-one</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.9664]</td>\n",
       "      <td>/tmp/rnn-mnist/run-20171207015933-train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BasicRNNCell, many-to-one</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.9576]</td>\n",
       "      <td>/tmp/rnn-mnist/run-20171207015939-train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      modelo num_units num_epochs exactitud (accuracy)  \\\n",
       "0    RNN simple, many-to-one       128         20             [0.9664]   \n",
       "1  BasicRNNCell, many-to-one       128         20             [0.9576]   \n",
       "\n",
       "                              train_logdir  \n",
       "0  /tmp/rnn-mnist/run-20171207015933-train  \n",
       "1  /tmp/rnn-mnist/run-20171207015939-train  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre_del_experimento = \"BasicRNNCell, many-to-one\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "root_logdir = \"/tmp/rnn-mnist\" \n",
    "train_logdir = \"{}/run-{}-train\".format(root_logdir, now)\n",
    "val_logdir = \"{}/run-{}-val\".format(root_logdir, now)\n",
    "train_file_writer = tf.summary.FileWriter(train_logdir, tf.get_default_graph())\n",
    "val_file_writer = tf.summary.FileWriter(val_logdir)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"Epoch  Train_loss  Train_acc   Val_loss    Val_acc\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 2048\n",
    "num_batches_per_epoch = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess: \n",
    "  \n",
    "  tf.global_variables_initializer().run()\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    for step in range(num_batches_per_epoch):\n",
    "      cur_step = epoch * num_batches_per_epoch + step\n",
    "      batch_images, batch_labels = mnist.train.next_batch(batch_size)\n",
    "      _, train_loss, train_acc, summary = sess.run([training_op, loss, accuracy, merged], \n",
    "                                                     feed_dict={x: batch_images, labels: batch_labels})\n",
    "      train_file_writer.add_summary(summary, cur_step + 1)\n",
    "      \n",
    "    val_loss, val_acc, summary = sess.run([loss, accuracy, merged], \n",
    "                                          feed_dict={x: mnist.validation.images, labels: mnist.validation.labels})\n",
    "    val_file_writer.add_summary(summary, cur_step )\n",
    "    print(\"{:04d}{:12.7f}{:12.7f}{:12.7f}{:12.7f}\".format\n",
    "          (epoch+1, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "  test_acc = sess.run([accuracy], \n",
    "                      feed_dict={x: mnist.test.images, labels: mnist.test.labels})\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"=====================================================\")\n",
    "\n",
    "train_file_writer.close()\n",
    "val_file_writer.close()\n",
    "\n",
    "## Añadimos nuestros resultados al dataframe\n",
    "idx = len(resultados)\n",
    "resultados.loc[idx] = [nombre_del_experimento,\n",
    "                       num_units,\n",
    "                       num_epochs,\n",
    "                       test_acc,\n",
    "                       train_logdir ]\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN *many-to-many* para clasificación de MNIST\n",
    "\n",
    "### 2.1. RNN *many-to-many* de una capa\n",
    "\n",
    "En la primera parte hemos empleado una RNN *many-to-one*, entrenada para clasificar el dígito correspondiente a una imagen MNIST interpretada como una secuencia de filas de pixeles. Ahora experimentaremos con una RNN que intente predecir el dígito de la imagen desde cada estado de la RNN.\n",
    "\n",
    "Para ello, en lugar de calcular los `logits` sólo desde el estado final, los calcularemos desde cada estado. Con ello, sus dimensiones serán ahora `[batch_size, n_steps, num_classes]` \n",
    "\n",
    "Haz los siguientes cambios:\n",
    "\n",
    "* En la sección `tf.name_scope(\"inputs\")` genera una versión de `labels` \"expandida\" por el número de pasos.\n",
    "```python\n",
    "labels_per_step = tf.reshape(tf.tile(labels, [1, num_steps]), [-1, num_classes])\n",
    "```\n",
    "* En la sección `tf.name_scope(\"output\")` añade el siguiente código para hacer predicciones en cada paso:\n",
    "```python\n",
    "    h_outputs = tf.reshape(h_outputs, [-1, num_units])\n",
    "    logits_per_step = c + tf.matmul(h_outputs, V)\n",
    "    logits_per_step_reshaped = tf.reshape(logits_per_step, [-1, num_steps, num_classes])\n",
    "    y_probs_per_step = tf.nn.softmax(logits_per_step_reshaped)\n",
    "    y_predictions_per_step = tf.argmax(y_probs_per_step, axis=2) \n",
    "```\n",
    "* En la sección `tf.name_scope(\"train\")` modifica la función de costo para que calcule la entropía cruzada usando `labels_per_step` y `logits_per_step`.\n",
    "\n",
    "En la ejecución solamente incluiremos la consulta a las predicciones y probabilidades por cada paso en el conjunto de prueba:\n",
    "```python\n",
    "  test_acc, test_probs, test_predictions = sess.run([accuracy, y_probs_per_step, y_predictions_per_step], \n",
    "                                                    feed_dict={x: mnist.test.images, labels: mnist.test.labels})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# DIMENSIONES DEL MODELO\n",
    "#########################\n",
    "\n",
    "# Longitud de las secuencias: cada imagen tiene 28 filas (alto)\n",
    "num_steps = mnist.train.images.shape[1]\n",
    "\n",
    "# Capa de entrada: cada paso será una fila de 28 pixeles (ancho)\n",
    "input_size = mnist.train.images.shape[2]\n",
    "\n",
    "# Número de unidades en las unidades ocultas\n",
    "num_units = 128\n",
    "\n",
    "# Número de clases: 10 dígitos diferentes\n",
    "num_classes = mnist.train.labels.shape[1]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "####################\n",
    "# TENSORES\n",
    "####################\n",
    "\n",
    "# Datos de entrada\n",
    "with tf.name_scope(\"inputs\"):\n",
    "  x = tf.placeholder(tf.float32, [None, num_steps, input_size, 1], name=\"x\")\n",
    "  labels = tf.placeholder(tf.float32, [None, num_classes], name=\"labels\")\n",
    "  labels_per_step = tf.reshape(tf.tile(labels, [1, num_steps]), [-1, num_classes])\n",
    "  x_reshaped = tf.reshape(x, [-1, num_steps, input_size], name=\"x_reshaped\")\n",
    "\n",
    "# Parámetros del modelo (pesos y biases)\n",
    "with tf.variable_scope(\"parameters\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "  U = tf.get_variable(\"U\", [input_size, num_units])\n",
    "  W = tf.get_variable(\"W\", [num_units, num_units])\n",
    "  V = tf.get_variable(\"V\", [num_units, num_classes])\n",
    "  \n",
    "with tf.variable_scope(\"parameters\", initializer=tf.constant_initializer(0.)):\n",
    "  b = tf.get_variable(\"b\", [num_units])\n",
    "  c = tf.get_variable(\"c\", [num_classes])\n",
    "  \n",
    "#########################\n",
    "# OPERACIONES DEL MODELO\n",
    "#########################\n",
    "\n",
    "'''\n",
    "def next_h(h_prev, x_t):\n",
    "  with tf.variable_scope(\"parameters\", reuse=True):\n",
    "    U = tf.get_variable(\"U\")\n",
    "    W = tf.get_variable(\"W\")\n",
    "    b = tf.get_variable(\"b\")\n",
    "  a_t = b + tf.matmul(h_prev, W) + tf.matmul(x_t, U)\n",
    "  h_t = tf.tanh(a_t)\n",
    "  return h_t\n",
    "'''\n",
    "\n",
    "with tf.name_scope(\"model\"):\n",
    "  with tf.name_scope(\"hidden\"):    \n",
    "    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "    h_outputs, h_state = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=x_reshaped, dtype=tf.float32)\n",
    "    '''\n",
    "    h_0 = tf.zeros([tf.shape(x)[0], num_units], name=\"h_0\")\n",
    "    h_outputs = tf.scan(next_h, \n",
    "                        tf.transpose(x_reshaped, [1,0,2]),  # [num_steps, batch_size, num_inputs])\n",
    "                        initializer=h_0,\n",
    "                        name=\"h_outputs\"\n",
    "                       )\n",
    "    h_state = h_outputs[-1]\n",
    "    h_outputs = tf.transpose(h_outputs, [1,0,2], name=\"reshaped_h_outputs\")  # [batch_size, num_steps, num_units]\n",
    "    '''\n",
    "    \n",
    "  with tf.name_scope(\"output\"):\n",
    "    logits = c + tf.matmul(h_state, V)\n",
    "    y_probs = tf.nn.softmax(logits)\n",
    "    y_predictions = tf.argmax(y_probs, axis=1)\n",
    "    h_outputs = tf.reshape(h_outputs, [-1, num_units])\n",
    "    \n",
    "    logits_per_step = c + tf.matmul(h_outputs, V)\n",
    "    logits_per_step_reshaped = tf.reshape(logits_per_step, [-1, num_steps, num_classes])\n",
    "    y_probs_per_step = tf.nn.softmax(logits_per_step_reshaped)\n",
    "    y_predictions_per_step = tf.argmax(y_probs_per_step, axis=2)\n",
    "    \n",
    "#########################\n",
    "# OPERACIONES DE ENTRENAMIENTO\n",
    "#########################\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "  #cost = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  cost = tf.nn.softmax_cross_entropy_with_logits(labels=labels_per_step, logits=logits_per_step)\n",
    "  loss = tf.reduce_mean(cost)\n",
    "  optimizer = tf.train.AdamOptimizer()\n",
    "  training_op = optimizer.minimize(loss)\n",
    "  \n",
    "\n",
    "#########################\n",
    "# OPERACIONES DE EVALUACIÓN Y VISUALIZACIÓN\n",
    "#########################\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "  y = tf.argmax(labels, axis=1)\n",
    "  correct = tf.nn.in_top_k(logits, y, 1)\n",
    "  accuracy = tf.reduce_mean(tf.to_float(correct))\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "  acc_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "  loss_summary = tf.summary.scalar('loss', loss)\n",
    "  merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Epoch  Train_loss  Train_acc   Val_loss    Val_acc\n",
      "-----------------------------------------------------\n",
      "0001   1.9686178   0.3168945   1.9758295   0.3318000\n",
      "0002   1.7462701   0.4487305   1.7369083   0.4428000\n",
      "0003   1.5496877   0.5810547   1.5074592   0.6120000\n",
      "0004   1.3473375   0.7285156   1.3414625   0.7398000\n",
      "0005   1.2465600   0.7963867   1.2379891   0.7962000\n",
      "0006   1.1631039   0.8330078   1.1623590   0.8300000\n",
      "0007   1.1291684   0.8432617   1.1167397   0.8472000\n",
      "0008   1.0863687   0.8691406   1.0680777   0.8758000\n",
      "0009   1.0732927   0.8632812   1.0484580   0.8764000\n",
      "0010   1.0432396   0.8842773   1.0132946   0.8958000\n",
      "0011   1.0002620   0.9018555   0.9954647   0.9012000\n",
      "0012   1.0242083   0.9023438   0.9773375   0.9020000\n",
      "0013   0.9926221   0.8964844   0.9651242   0.9070000\n",
      "0014   0.9669784   0.9082031   0.9534706   0.9114000\n",
      "0015   0.9406610   0.9257812   0.9383736   0.9164000\n",
      "0016   0.9428278   0.9165039   0.9250610   0.9222000\n",
      "0017   0.9317504   0.9287109   0.9169464   0.9228000\n",
      "0018   0.9563008   0.9038086   0.9092028   0.9288000\n",
      "0019   0.9287406   0.9160156   0.9013879   0.9140000\n",
      "0020   0.9053673   0.9179688   0.8849894   0.9308000\n",
      "-----------------------------------------------------\n",
      "Test accuracy: 0.9286\n",
      "=====================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>exactitud (accuracy)</th>\n",
       "      <th>train_logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN simple, many-to-one</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.9664]</td>\n",
       "      <td>/tmp/rnn-mnist/run-20171207015933-train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BasicRNNCell, many-to-one</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.9576]</td>\n",
       "      <td>/tmp/rnn-mnist/run-20171207015939-train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BasicRNNCell, many-to-many</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>/tmp/rnn-mnist/run-20171207020736-train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       modelo num_units num_epochs exactitud (accuracy)  \\\n",
       "0     RNN simple, many-to-one       128         20             [0.9664]   \n",
       "1   BasicRNNCell, many-to-one       128         20             [0.9576]   \n",
       "2  BasicRNNCell, many-to-many       128         20               0.9286   \n",
       "\n",
       "                              train_logdir  \n",
       "0  /tmp/rnn-mnist/run-20171207015933-train  \n",
       "1  /tmp/rnn-mnist/run-20171207015939-train  \n",
       "2  /tmp/rnn-mnist/run-20171207020736-train  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre_del_experimento = \"BasicRNNCell, many-to-many\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "root_logdir = \"/tmp/rnn-mnist\" \n",
    "train_logdir = \"{}/run-{}-train\".format(root_logdir, now)\n",
    "val_logdir = \"{}/run-{}-val\".format(root_logdir, now)\n",
    "train_file_writer = tf.summary.FileWriter(train_logdir, tf.get_default_graph())\n",
    "val_file_writer = tf.summary.FileWriter(val_logdir)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"Epoch  Train_loss  Train_acc   Val_loss    Val_acc\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 2048\n",
    "num_batches_per_epoch = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess: \n",
    "  \n",
    "  tf.global_variables_initializer().run()\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    for step in range(num_batches_per_epoch):\n",
    "      cur_step = epoch * num_batches_per_epoch + step\n",
    "      batch_images, batch_labels = mnist.train.next_batch(batch_size)\n",
    "      _, train_loss, train_acc, summary = sess.run([training_op, loss, accuracy, merged], \n",
    "                                                     feed_dict={x: batch_images, labels: batch_labels})\n",
    "      train_file_writer.add_summary(summary, cur_step + 1)\n",
    "      \n",
    "    val_loss, val_acc, summary = sess.run([loss, accuracy, merged], \n",
    "                                          feed_dict={x: mnist.validation.images, labels: mnist.validation.labels})\n",
    "    val_file_writer.add_summary(summary, cur_step )\n",
    "    print(\"{:04d}{:12.7f}{:12.7f}{:12.7f}{:12.7f}\".format\n",
    "          (epoch+1, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "  #test_acc = sess.run([accuracy], \n",
    "  #                    feed_dict={x: mnist.test.images, labels: mnist.test.labels})\n",
    "  test_acc, test_probs, test_predictions = sess.run([accuracy, y_probs_per_step, y_predictions_per_step], \n",
    "                                                    feed_dict={x: mnist.test.images, labels: mnist.test.labels})\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"=====================================================\")\n",
    "\n",
    "train_file_writer.close()\n",
    "val_file_writer.close()\n",
    "\n",
    "## Añadimos nuestros resultados al dataframe\n",
    "idx = len(resultados)\n",
    "resultados.loc[idx] = [nombre_del_experimento,\n",
    "                       num_units,\n",
    "                       num_epochs,\n",
    "                       test_acc,\n",
    "                       train_logdir ]\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos las predicciones para cada uno de los pasos en una imagen cualquiera del conjunto de pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc711085be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas                                                       Predicción     Probabilidad\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        2         0.1113448\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        1         0.1145547\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        1         0.1185016\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        1         0.1234492\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        7         0.1447916\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0]        0         0.4434519\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0]        5         0.3027963\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0]        5         0.5446369\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.6445640\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.7298463\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.8552077\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9135811\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9500619\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9692543\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9697380\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]        5         0.9429075\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]        5         0.9153816\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]        5         0.9535595\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]        5         0.9774329\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]        5         0.9786137\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0]        5         0.9778509\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9807490\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9842632\n",
      "[0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9845166\n",
      "[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9835278\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9873136\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9864921\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]        5         0.9829096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.axis('off')\n",
    "\n",
    "img_idx = 23  # Escoge el número de imagen que desees\n",
    "mnist_image = mnist.test.images[img_idx].reshape(28,28)\n",
    "binarized_img = (mnist_image > 0.5).astype(int)\n",
    "plt.imshow(mnist.test.images[img_idx].reshape(28,28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"{:60}{:15}{:10}\".format(\n",
    "  \"Filas\", \n",
    "  \"Predicción\",\n",
    "  \"Probabilidad\"\n",
    "     ))\n",
    "  \n",
    "for row in np.arange(28):\n",
    "  print(\"{}{:9d}{:18.7f}\".format(\n",
    "    binarized_img[row], \n",
    "    test_predictions[img_idx,row],\n",
    "    np.max(test_probs[img_idx,row])\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Añadiendo profundidad a la capa oculta\n",
    "\n",
    "Para usar una capa oculta con 2 (o más celdas) de profundidad, haz las siguientes modificaciones:\n",
    "```python\n",
    "...\n",
    "# Número de capas RNN\n",
    "num_rnn_layers = 2\n",
    "...\n",
    "  with tf.name_scope(\"hidden\"):\n",
    "    def rnn_cell():\n",
    "      return tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "    multilayer_rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell() for _ in range(num_rnn_layers)])\n",
    "    h_outputs, h_states = tf.nn.dynamic_rnn(cell=multilayer_rnn_cell, inputs=x_reshaped, dtype=tf.float32)\n",
    "    h_state = h_states[-1]\n",
    "...\n",
    "```\n",
    "\n",
    "Entrena el modelo por 20 épocas y visualiza las predicciones para cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_del_experimento = \"BasicRNNCell, many-to-many, 2 rnn layers\"\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
